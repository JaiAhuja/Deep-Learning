{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2af5aa67",
   "metadata": {},
   "source": [
    "In this notebook, we'll implement a L-Layer deep model on MNIST dataset using Keras. The dataset contains tens of thousands of scanned images of handwritten digits, together with their correct classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ad8add4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing relevant libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import scipy\n",
    "\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
    "from keras import regularizers\n",
    "\n",
    "np.random.seed(7)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e432541b",
   "metadata": {},
   "source": [
    "The MNIST dataset we use here is \"mnist.pkl.gz\" which is divided into training, validation and test data. The following function \"load_data()\" unpacks the file and eztracts the training, validation and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bb13c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    f = gzip.open(\"mnist.pkl.gz\", \"rb\")\n",
    "    f.seek(0)\n",
    "    training_data, validation_data, test_data = pickle.load(f, encoding=\"latin1\")\n",
    "    f.close()\n",
    "    return (training_data, validation_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32e622a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how the dataset looks\n",
    "training_data, validation_data, test_data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d048e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       " array([5, 0, 4, ..., 8, 4, 8], dtype=int64))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e244baea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The feature dataset is:[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "The target dataset is:[5 0 4 ... 8 4 8]\n",
      "The number of examples in the training dataset is:50000\n",
      "The number of points in a single input is:784\n"
     ]
    }
   ],
   "source": [
    "print(\"The feature dataset is:\" + str(training_data[0]))\n",
    "print(\"The target dataset is:\" + str(training_data[1]))\n",
    "print(\"The number of examples in the training dataset is:\" + str(len(training_data[0])))\n",
    "print(\"The number of points in a single input is:\" + str(len(training_data[0][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21011f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The target variable is converted to a one hot matrix. We use the function one_hot to convert the target dataset to one hot encoding\n",
    "def one_hot(j):\n",
    "    n = j.shape[0]\n",
    "    new_array = np.zeros((10,n))\n",
    "    index = 0\n",
    "    for res in j:\n",
    "        new_array[res][index] = 1.0\n",
    "        index += 1\n",
    "    return new_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b01cdd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([0,1,2,3,4,5,6,7,8,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a43181d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e5b6eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_wrapper():\n",
    "    tr_d, va_d, te_d = load_data()\n",
    "    \n",
    "    training_inputs = np.array(tr_d[0][:]).T\n",
    "    training_results = np.array(tr_d[1][:])\n",
    "    train_set_y = one_hot(training_results)\n",
    "    \n",
    "    validation_inputs = np.array(va_d[0][:]).T\n",
    "    validation_results = np.array(va_d[1][:])\n",
    "    validation_set_y = one_hot(validation_results)\n",
    "    \n",
    "    test_inputs = np.array(te_d[0][:]).T\n",
    "    test_results = np.array(te_d[1][:])\n",
    "    test_set_y = one_hot(test_results)\n",
    "    \n",
    "    return (training_inputs, train_set_y, validation_inputs, validation_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a955c946",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x, train_set_y, test_set_x, test_set_y = data_wrapper()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4d90fa",
   "metadata": {},
   "source": [
    "For implementing in Keras, the input training and input dataset are supposed to have shape (m,n) where m is the number of training samples and n is the number of parts in a single input.\n",
    "Hence, let's create the desired dataset shapes by taking the transpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4f4e091",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x = train_set_x.T\n",
    "train_set_y = train_set_y.T\n",
    "test_set_x = test_set_x.T\n",
    "test_set_y = test_set_y.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f36e3ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_x shape:(50000, 784)\n",
      "train_set_y shape:(50000, 10)\n",
      "test_set_x shape:(10000, 784)\n",
      "test_set_y shape:(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Shape of the datasets\n",
    "print(\"train_set_x shape:\" + str(train_set_x.shape))\n",
    "print(\"train_set_y shape:\" + str(train_set_y.shape))\n",
    "print(\"test_set_x shape:\" + str(test_set_x.shape))\n",
    "print(\"test_set_y shape:\" + str(test_set_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04f4a9cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualising the dataset\n",
    "index = 1000\n",
    "k = train_set_x[index,:]\n",
    "k = k.reshape((28,28))\n",
    "plt.title(\"label is {label}\".format(label=training_data[1][index]))\n",
    "plt.imshow(k,cmap = \"gray\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46da23e",
   "metadata": {},
   "source": [
    "Keras is a framework. So, to implement a neural network model in Keras, we first create an instance of Sequential().\n",
    "\n",
    "The Sequential model is a linear stack of layers. We then keep adding Dense layers that are fully connected layers as we desire.\n",
    "\n",
    "We can also add regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af65c6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model\n",
    "nn_model = Sequential()\n",
    "nn_model.add(Dense(35, input_dim = 784, activation = \"relu\"))\n",
    "nn_model.add(Dropout(0.3))\n",
    "nn_model.add(Dense(21, activation = \"relu\"))\n",
    "nn_model.add(Dense(10, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a649ae8",
   "metadata": {},
   "source": [
    "Before we run the model on the training datasets, we compile the model in which we define various things like loss function, the optimizer and the evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5632915",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model.compile(loss= \"categorical_crossentropy\", optimizer= \"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2b663b",
   "metadata": {},
   "source": [
    "Now, to fit the model on the training input and training target dataset, we run the following command using a minibatch of size 10 and 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91dcee78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5000/5000 [==============================] - 6s 1ms/step - loss: 0.5276 - accuracy: 0.8365\n",
      "Epoch 2/10\n",
      "5000/5000 [==============================] - 6s 1ms/step - loss: 0.3196 - accuracy: 0.9028\n",
      "Epoch 3/10\n",
      "5000/5000 [==============================] - 5s 1ms/step - loss: 0.2814 - accuracy: 0.9131\n",
      "Epoch 4/10\n",
      "5000/5000 [==============================] - 6s 1ms/step - loss: 0.2586 - accuracy: 0.9204\n",
      "Epoch 5/10\n",
      "5000/5000 [==============================] - 6s 1ms/step - loss: 0.2386 - accuracy: 0.9260\n",
      "Epoch 6/10\n",
      "5000/5000 [==============================] - 5s 1ms/step - loss: 0.2336 - accuracy: 0.9261\n",
      "Epoch 7/10\n",
      "5000/5000 [==============================] - 5s 1ms/step - loss: 0.2246 - accuracy: 0.9297\n",
      "Epoch 8/10\n",
      "5000/5000 [==============================] - 6s 1ms/step - loss: 0.2133 - accuracy: 0.9326\n",
      "Epoch 9/10\n",
      "5000/5000 [==============================] - 5s 1ms/step - loss: 0.2076 - accuracy: 0.9348\n",
      "Epoch 10/10\n",
      "5000/5000 [==============================] - 6s 1ms/step - loss: 0.2065 - accuracy: 0.9361\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a38160cd90>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model.fit(train_set_x, train_set_y, epochs = 10, batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59962c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 2s 903us/step - loss: 0.1038 - accuracy: 0.9694\n",
      "\n",
      "accuracy: 96.94%\n"
     ]
    }
   ],
   "source": [
    "scores_train = nn_model.evaluate(train_set_x, train_set_y)\n",
    "print(\"\\n%s: %.2f%%\" % (nn_model.metrics_names[1], scores_train[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedaf492",
   "metadata": {},
   "source": [
    "`Inference:` We can see that the model has 97% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64cad64c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 8, 6, ..., 5, 6, 8], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's make the predictions on the test dataset\n",
    "predictions = nn_model.predict(test_set_x)\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1442bcc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 944us/step - loss: 0.1338 - accuracy: 0.9623\n",
      "\n",
      "accuracy: 96.23%\n"
     ]
    }
   ],
   "source": [
    "scores_test = nn_model.evaluate(test_set_x, test_set_y)\n",
    "print(\"\\n%s: %.2f%%\" % (nn_model.metrics_names[1], scores_test[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66ec229",
   "metadata": {},
   "source": [
    "`Inference:` We can see that the model has ~96% accuracy on the training dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
